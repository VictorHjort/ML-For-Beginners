{"cells":[{"cell_type":"markdown","metadata":{},"source":["3-Classification\n","================\n","\n","**Author:** Cumhur Erkut\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Introduction to classification <code>[0/6]</code>\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Best resources for ML Algorithms:\n","\n","-   [https://saturncloud.io/glossary/](https://saturncloud.io/glossary/)\n","-   [2 Machine Learning General - Google Slides](https://docs.google.com/presentation/d/1qSOwBrjEmZTXQqNqB9XRAV7QsB6SJrLZ4pZBCkpvzyA/edit#slide=id.gf297669038_0_129)\n","\n","This section is all about balancing and resaving the data.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Pre-lecture quiz\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Q: Multiclass. Given ingredients, which cousine? \n","\n","-   [X]Â Classification, regression relationship\n","-   [X]Â First step: analyze and balance your data\\*\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Hello classifier\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["-   **multiclass:** Given a batch of indgedient, which of these cuisines (multiple class) will the data fit?\n","\n","-   [ ] Define classification\n","-   [ ] Take a moment to imagine a dataset about cuisines\n","    -   [ ] What can a binary model answer?\n","        \n","            given a present of a grocery bag full of star anise, artichokes, cauliflower, and horseradish, can we create a typical Indian dish?\n","    -   [ ] What can a multiclass model answer?\n","        \n","            Which cuisine is likely to use fenugreek?\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise: (Clean and balance) your data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    #! pip install imblearn --yes\n","    import pandas as pd\n","    import matplotlib as mpl\n","    import matplotlib.pyplot as plt\n","    import numpy as np\n","    from imblearn.over_sampling import SMOTE\n","\n","    df = pd.read_csv('../data/cuisines.csv')\n","    df.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise: Learning about cuisines\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.cuisine.value_counts().plot.barh()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    thai_df = df[(df.cuisine == \"thai\")]\n","    japanese_df = df[(df.cuisine == \"japanese\")]\n","    chinese_df = df[(df.cuisine == \"chinese\")]\n","    indian_df = df[(df.cuisine == \"indian\")]\n","    korean_df = df[(df.cuisine == \"korean\")]\n","    \n","    print(f'thai df: {thai_df.shape}')\n","    print(f'japanese df: {japanese_df.shape}')\n","    print(f'chinese df: {chinese_df.shape}')\n","    print(f'indian df: {indian_df.shape}')\n","    print(f'korean df: {korean_df.shape}')\n","    "]},{"cell_type":"markdown","metadata":{},"source":["What are the typical ingredients per cuisine? \n","First, clean out recurrent data that creates confusion between cuisines."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    def create_ingredient_df(df):\n","        ingredient_df = df.T.drop(['cuisine','Unnamed: 0']).sum(axis=1).to_frame('value')\n","        ingredient_df = ingredient_df[(ingredient_df.T != 0).any()]\n","        ingredient_df = ingredient_df.sort_values(by='value', ascending=False, inplace=False)\n","        return ingredient_df\n","\n","    thai_ingredient_df = create_ingredient_df(thai_df)\n","    thai_ingredient_df.head(10).plot.barh()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","    japanese_ingredient_df = create_ingredient_df(japanese_df)\n","#    japanese_ingredient_df.head(10).plot.barh()\n","    chinese_ingredient_df = create_ingredient_df(chinese_df)\n","#    chinese_ingredient_df.head(10).plot.barh()\n","    indian_ingredient_df = create_ingredient_df(indian_df)\n","#    indian_ingredient_df.head(10).plot.barh()\n","    korean_ingredient_df = create_ingredient_df(korean_df)\n","#    korean_ingredient_df.head(10).plot.barh()"]},{"cell_type":"markdown","metadata":{},"source":["Drop Unnamed column and most frequent ingredients"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","    feature_df = df.drop(['cuisine', 'Unnamed: 0','rice', 'garlic', 'ginger'], axis=1)\n","    labels_df = df.cuisine #.unique()\n","    feature_df.head()\n"]},{"cell_type":"markdown","metadata":{},"source":["### Balance the dataset using SMOTE\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Balance data with SMOTE oversampling to the highest class. Read more here: [https://imbalanced-learn.org/dev/references/generated/imblearn.over_sampling.SMOTE.html](https://imbalanced-learn.org/dev/references/generated/imblearn.over_sampling.SMOTE.html)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","    oversample = SMOTE()\n","    transformed_feature_df, transformed_label_df = oversample.fit_resample(feature_df, labels_df)\n","\n","    print(f'new label count: {transformed_label_df.value_counts()}')\n","    print(f'old label count: {df.cuisine.value_counts()}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","    transformed_df = pd.concat([transformed_label_df,transformed_feature_df],axis=1, join='outer')\n","    transformed_df.head()\n","    transformed_df.info()\n","    transformed_df.to_csv(\"../data/cleaned_cuisines.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["### ðŸš€Challenge, Post-lecture quiz, Review & Self Study\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## 2. More classifiers: [Logistic Regression](https://paperswithcode.com/method/logistic-regression) (and [Support Vector](https://paperswithcode.com/method/svm) Classifiers)"]},{"cell_type":"markdown","metadata":{},"source":["    Assumption: a cleaned_cuisines.csv file exists in the root /data folder for these four lessons.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Exercise: predict a national cuisine\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    import pandas as pd\n","    cuisines_df = pd.read_csv(\"../data/cleaned_cuisines.csv\")\n","    cuisines_df.head()\n","\n","    from sklearn.linear_model import LogisticRegression\n","    from sklearn.model_selection import train_test_split, cross_val_score\n","    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n","    from sklearn.svm import SVC\n","    import numpy as np\n","\n","    cuisines_label_df = cuisines_df['cuisine']\n","    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n","\n","    X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    # Logistic Regression\n","    lr = LogisticRegression(multi_class='ovr',solver='liblinear')\n","    model = lr.fit(X_train, np.ravel(y_train))\n","    \n","    accuracy = model.score(X_test, y_test)\n","    print (\"Accuracy is {}\".format(accuracy))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    # Test classification instance \n","    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')\n","    print(f'cuisine: {y_test.iloc[50]}')\n","\n","    test= X_test.iloc[50].values.reshape(-1, 1).T\n","    proba = model.predict_proba(test)\n","    classes = model.classes_\n","    resultdf = pd.DataFrame(data=proba, columns=classes)\n","    \n","    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])\n","    topPrediction.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    y_pred = model.predict(X_test)\n","    print(classification_report(y_test,y_pred))"]},{"cell_type":"markdown","metadata":{},"source":["### ðŸš€Challenge, Post-lecture quiz, Review & Self Study\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Yet other classifiers\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["We assume that you have completed the previous lessons and have a cleaned dataset in your \\`data\\` folder called <u>cleaned\\_cuisines.csv</u>.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### A classification map ([interactive version in your browser](https://scikit-learn.org/stable/tutorial/machine_learning_map/)): The plan, hacked code\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["![img](../3-Classifiers-2/images/map.png)\n","\n","-   [ ] Linear SVC\n","-   [ ] KNN\n","-   [ ] SVC\n","-   [ ] Ensemble (FRST, ADA)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    import pandas as pd\n","    cuisines_df = pd.read_csv(\"../data/cleaned_cuisines.csv\")\n","    cuisines_label_df = cuisines_df['cuisine']\n","    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n","    \n","    from sklearn.neighbors import KNeighborsClassifier\n","    from sklearn.linear_model import LogisticRegression\n","    from sklearn.svm import SVC\n","    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","    from sklearn.model_selection import train_test_split, cross_val_score\n","    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n","    import numpy as np\n","    \n","    X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    C = 10\n","    # Create different classifiers.\n","    classifiers = {\n","        'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0),\n","        'KNN classifier': KNeighborsClassifier(C),\n","        'SVC': SVC(),\n","        'RFST': RandomForestClassifier(n_estimators=100),\n","        'ADA': AdaBoostClassifier(n_estimators=100)  \n","    }\n","\n","    import warnings\n","    # Filter out user warnings due to SVC and KNN usesr warnings\n","    warnings.filterwarnings(\"ignore\", category=UserWarning)\n","    \n","    n_classifiers = len(classifiers)\n","    \n","    for index, (name, classifier) in enumerate(classifiers.items()):\n","        classifier.fit(X_train, np.ravel(y_train))\n","        X_test = np.ascontiguousarray(X_test) # fixes KNN c_contiguous array error\n","        y_pred = classifier.predict(X_test)\n","        accuracy = accuracy_score(y_test, y_pred)\n","        print(\"Accuracy (train) for %s: %0.1f%% \" % (name, accuracy * 100))\n","        print(classification_report(y_test,y_pred))"]},{"cell_type":"markdown","metadata":{},"source":["### ðŸš€Challenge, Post-lecture quiz, Review & Self Study\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["There's a lot of jargon in these lessons, so take a minute to reviewÂ [this list](https://docs.microsoft.com/dotnet/machine-learning/resources/glossary?WT.mc_id=academic-77952-leestott)Â of useful terminology!\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Applied ML: build a web app\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["-   [ ] How to build a model and save it as an Onnx model\n","-   [ ] How to use Netron to inspect the model\n","-   [ ] How to use your model in a web app for inference\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### [Pre-lecture quiz](https://gray-sand-07a10f403.1.azurestaticapps.net/quiz/25/)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Build your model\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["*build a basic JavaScript-based system for inference. First, however, you need to train a model and convert it for use with Onnx*.\n","\n","[Open Neural Network Exchange (ONNX)](https://onnx.ai/)Â is an open standard format for representing machine learning models. See the basit tutorials at [https://github.com/onnx/tutorials](https://github.com/onnx/tutorials)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Exercise: Train classification model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    # ! mamba install skl2onnx==1.15.0\n","    import pandas as pd\n","\n","    data = pd.read_csv('../data/cleaned_cuisines.csv')\n","    X = data.iloc[:,2:]\n","    y = data[['cuisine']]\n","\n","    from sklearn.model_selection import train_test_split\n","    from sklearn.svm import SVC\n","    from sklearn.model_selection import cross_val_score\n","    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report\n","    \n","    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# SVC\n","    model = SVC(kernel='linear', C=10, probability=True,random_state=0)\n","    model.fit(X_train,y_train.values.ravel())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    y_pred = model.predict(X_test)\n","    print(classification_report(y_test,y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    # ONNX EXPORT\n","\n","    from skl2onnx import convert_sklearn\n","    from skl2onnx.common.data_types import FloatTensorType\n","    \n","    initial_type = [('float_input', FloatTensorType([None, 380]))]\n","    # ** Note the options: nocl: no class info embedded (smaller filesize). zipmap: list of dictionaries \n","    options = {id(model): {'nocl': True, 'zipmap': False}}\n","    \n","    onx = convert_sklearn(model, initial_types=initial_type, options=options)\n","    with open(\"./model.onnx\", \"wb\") as f:\n","        f.write(onx.SerializeToString())\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### OPTIONAL View your model using [netron](https://github.com/lutzroeder/netron)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### MAC\n","    # brew install netron\n","    # open model.onnx\n","#### WIN\n","    winget install -s winget netron\n","\n","![img](../4-Applied/images/netron.png)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Build a recommender web application (by writing index.html and js, then running http-server) then test\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Better way: FastAPI! Wait for it (4-Web-App)\n","Best to check [../4-Applied/solution/index.html](../4-Applied/solution/index.html)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### ðŸš€Challenge, Post-lecture quiz, Review & Self Study\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Convolutional Neural Nets\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Interactively: [https://poloclub.github.io/cnn-explainer/](https://poloclub.github.io/cnn-explainer/)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Using scikit-learn MLP as a proxy to CNN\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["    After [Training CNN with Images in Sklearn Neural Net: A Step-by-Step Guide | Saturn Cloud Blog](https://saturncloud.io/blog/training-cnn-with-images-in-sklearn-neural-net-a-stepbystep-guide/)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","    from sklearn.datasets import load_digits\n","    from skimage import color\n","    from skimage.transform import resize\n","    # Load sample images: other datasets load_digits() and load_iris()\n","    X,y = load_digits(return_X_y=True)\n","    # X = dataset.images\n","    # y = dataset.target\n","    \n","    # Commented parts for possible preprocessing using scikit-image\n","    # Preprocessing: resize, grayscale, normalize pixel values etc\n","    # import numpy as np\n","    # from sklearn.preprocessing import StandardScaler\n","    \n","    # Resize images\n","    # X_resized = np.array([resize(image, (64, 64), anti_aliasing=True) for image in X])\n","    # Convert to grayscale\n","    # X_gray = np.array([color.rgb2gray(image) for image in X_resized])\n","    \n","    # Normalize pixel values\n","    # scaler = StandardScaler()\n","    # X_scaled = scaler.fit_transform(X_gray.reshape(-1, 64 * 64))\n","    \n","    # Train - test split\n","    from sklearn.model_selection import train_test_split\n","    \n","    # Split data into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Model: we fake a MLP with relu activation as CNN\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    from sklearn.neural_network import MLPClassifier\n","    \n","    # Define MLPClassifier as CNN: 8x8 is flattened in the input layer, then progressively shrinked\n","    mlp = MLPClassifier(hidden_layer_sizes=(32, 16), activation='relu', solver='adam', max_iter=500)\n","    \n","    # Compile and fit the model\n","    mlp.fit(X_train, y_train)\n","    score = mlp.score(X_test, y_test)\n","    print(f\"Accuracy: {score}\")\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Improve the Model Performance?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["    # Increase model complexity\n","    mlp = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=1000)\n","    mlp.fit(X_train, y_train)\n","    score = mlp.score(X_test, y_test)\n","    print(f\"Accuracy: {score}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","    # Use SGD optimizer\n","    mlp = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='sgd', max_iter=500)\n","    mlp.fit(X_train, y_train)\n","    score = mlp.score(X_test, y_test)\n","    print(f\"Accuracy: {score}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","    # Use SGD optimizer with a low initial learning rate\n","    mlp = MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='sgd', learning_rate_init=0.001, max_iter=500)\n","    mlp.fit(X_train, y_train)\n","    score = mlp.score(X_test, y_test)\n","    print(f\"Accuracy: {score}\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Data augmentation (requires skimage)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Assignment: Parametrization of classification algorithms\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["There are a lot of parameters that are set by default when working with these classifiers. Intellisense in VS Code can help you dig into them. Adopt one of the ML Classification Techniques in this lesson and retrain models tweaking various parameter values. \n","\n","Build a notebook explaining why some changes help the model quality while others degrade it. Be detailed in your answer. \n","\n","For example, \n","\n","-   In [linear SVC](https://scikit-learn.org/stable/modules/svm.html#classification), \n","    -   Increasing C results in better model quality\n","    \n","    -   Increasing max iterations results in better model quality\n","    \n","    -   Increasing tolerance results in worse model quality\n","\n","-   In [KNN](https://saturncloud.io/glossary/knn/), \n","    -   Increasing n\\_neighbors results in better model quality\n","    \n","    -   Increasing p results in better model quality\n","    \n","    -   Increasing tolerance results in worse model quality\n","\n"]}],"metadata":[["org"],null,null],"nbformat":4,"nbformat_minor":0}
